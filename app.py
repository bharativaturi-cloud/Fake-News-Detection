# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OWwwKoKNlmbJH-8vNUatu3u98Fzbpwn5
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit
import streamlit as st
import pandas as pd
import joblib
import string
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
import numpy as np
import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# Download NLTK stopwords if not available
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

from nltk.corpus import stopwords

# Set page config
st.set_page_config(
    page_title="Fake News Detector",
    page_icon="ðŸ“°",
    layout="wide"
)

# Load the trained model
@st.cache_resource
def load_model():
    return joblib.load('/content/fake_news_model.pkl')

model = load_model()

# Define text preprocessing functions
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

def remove_stopwords(text):
    # Use both sklearn and nltk stopwords for compatibility
    custom_stopwords = ENGLISH_STOP_WORDS.union(set(stopwords.words('english')))
    words = text.split()
    filtered_words = [word for word in words if word not in custom_stopwords]
    return ' '.join(filtered_words)

def preprocess_text(text):
    text = str(text).lower()
    text = remove_punctuation(text)
    text = remove_stopwords(text)
    return text

# App title and description
st.title("ðŸ“° Fake News Detector")
st.markdown("""
This app uses a machine learning model to detect fake news articles.
You can either enter a news headline or upload a CSV file with multiple articles for analysis.
""")

# Create tabs for different functionalities
tab1, tab2 = st.tabs(["Single Article", "Batch Processing"])

with tab1:
    st.header("Analyze Single News Article")
    headline = st.text_area("Enter a news headline or article text:", height=100)

    if st.button("Analyze Article", type="primary"):
        if headline:
            # Preprocess the text
            processed_text = preprocess_text(headline)

            # Make prediction
            prediction = model.predict([processed_text])[0]
            prob_fake = model.predict_proba([processed_text])[0][0]
            prob_true = model.predict_proba([processed_text])[0][1]
            confidence = prob_fake if prediction == 'fake' else prob_true

            # Display results
            st.subheader("Analysis Results")

            if prediction == 'fake':
                st.error(f"Prediction: FAKE NEWS")
            else:
                st.success(f"Prediction: REAL NEWS")

            st.info(f"Confidence: {round(confidence * 100, 2)}%")

            # Show probability breakdown
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Probability of being FAKE", f"{round(prob_fake * 100, 2)}%")
            with col2:
                st.metric("Probability of being REAL", f"{round(prob_true * 100, 2)}%")
        else:
            st.warning("Please enter a news headline or article text.")

with tab2:
    st.header("Batch Process Multiple Articles")
    uploaded_file = st.file_uploader("Upload a CSV file with a 'text' column:", type="csv")

    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)

        if 'text' in df.columns:
            st.success("File uploaded successfully!")
            st.dataframe(df.head(), use_container_width=True)

            if st.button("Process Articles", type="primary"):
                with st.spinner("Processing articles..."):
                    # Preprocess text
                    df['processed_text'] = df['text'].apply(preprocess_text)

                    # Make predictions
                    predictions = model.predict(df['processed_text'])
                    probs = model.predict_proba(df['processed_text'])

                    # Map predictions to labels
                    label_map = {'fake': 'FAKE', 'true': 'REAL'}
                    df['prediction'] = [label_map.get(pred, pred) for pred in predictions]

                    # Add confidence scores
                    df['confidence'] = probs.max(axis=1) * 100
                    df['fake_probability'] = probs[:, 0] * 100
                    df['real_probability'] = probs[:, 1] * 100

                    # Display results
                    st.subheader("Analysis Results")
                    st.dataframe(df[['text', 'prediction', 'confidence']], use_container_width=True)

                    # Download button
                    csv = df.to_csv(index=False)
                    st.download_button(
                        label="Download Results as CSV",
                        data=csv,
                        file_name="fake_news_predictions.csv",
                        mime="text/csv"
                    )
        else:
            st.error("The uploaded CSV file must contain a 'text' column.")

# Add some information about the model
st.sidebar.header("About")
st.sidebar.info("""
This fake news detection model was built using:
- Logistic Regression classifier
- TF-IDF for text vectorization
- Trained on a dataset of ~45,000 news articles
""")

st.sidebar.header("How to Use")
st.sidebar.markdown("""
1. For single articles: Paste the text in the input box and click 'Analyze Article'
2. For batch processing: Upload a CSV file with a 'text' column containing articles
""")